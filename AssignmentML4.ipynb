{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLEASE NOTE: Please run this notebook OUTSIDE a Spark notebook as it should run in a plain Default Python 3.6 Free Environment\n",
    "\n",
    "This is the last assignment for the Coursera course \"Advanced Machine Learning and Signal Processing\"\n",
    "\n",
    "Just execute all cells one after the other and you are done - just note that in the last one you should update your email address (the one you've used for coursera) and obtain a submission token, you get this from the programming assignment directly on coursera.\n",
    "\n",
    "Please fill in the sections labelled with \"###YOUR_CODE_GOES_HERE###\"\n",
    "\n",
    "The purpose of this assignment is to learn how feature engineering boosts model performance. You will apply Discrete Fourier Transformation on the accelerometer sensor time series and therefore transforming the dataset from the time to the frequency domain.\n",
    "\n",
    "After that, you’ll use a classification algorithm of your choice to create a model and submit the new predictions to the grader. Done.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "package = \"systemml\"\n",
    "try:\n",
    "    __import__package\n",
    "except:\n",
    "    #! pip install systemml\n",
    "    os.system(\"pip install \"+ package)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "def printmd(string):\n",
    "    display(Markdown('# <span style=\"color:red\">'+string+'</span>'))\n",
    "\n",
    "if ('sc' in locals() or 'sc' in globals()):\n",
    "    printmd('<<<<<!!!!! It seems that you are running in a IBM Watson Studio Apache Spark Notebook. Please run it in an IBM Watson Studio Default Runtime (without Apache Spark) !!!!!>>>>>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "package = \"pyspark==2.4.5\"\n",
    "try:\n",
    "    __import__package\n",
    "except:\n",
    "    #! pip install systemml\n",
    "    os.system(\"pip install \"+ package)\n",
    "\n",
    "#!pip install pyspark==2.4.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting https://github.com/IBM/skillsnetwork/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true\n",
      "  Using cached https://github.com/IBM/skillsnetwork/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true (9.9 MB)\n",
      "Requirement already satisfied (use --upgrade to upgrade): systemml==1.3.0 from https://github.com/IBM/skillsnetwork/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true in c:\\programdata\\anaconda3\\lib\\site-packages\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from systemml==1.3.0) (1.19.5)\n",
      "Requirement already satisfied: scipy>=0.15.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from systemml==1.3.0) (1.4.1)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (from systemml==1.3.0) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (from systemml==1.3.0) (0.22.1)\n",
      "Requirement already satisfied: Pillow>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from systemml==1.3.0) (7.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->systemml==1.3.0) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas->systemml==1.3.0) (2019.3)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn->systemml==1.3.0) (0.14.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->systemml==1.3.0) (1.15.0)\n",
      "Building wheels for collected packages: systemml\n",
      "  Building wheel for systemml (setup.py): started\n",
      "  Building wheel for systemml (setup.py): finished with status 'done'\n",
      "  Created wheel for systemml: filename=systemml-1.3.0-py3-none-any.whl size=9882974 sha256=7b131dadaf1c6517f4d9c439f943f504fa3d489c48b6e4e657d67422421289f5\n",
      "  Stored in directory: c:\\users\\bilgi\\appdata\\local\\pip\\cache\\wheels\\b5\\f3\\65\\f44f93bcc9afc8fec7730345c4697654213f4c992e4608ac56\n",
      "Successfully built systemml\n"
     ]
    }
   ],
   "source": [
    "!pip install https://github.com/IBM/skillsnetwork/blob/master/systemml-1.3.0-SNAPSHOT-python.tar.gz?raw=true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext, SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, DoubleType, IntegerType, StringType\n",
    "sc = SparkContext.getOrCreate(SparkConf().setMaster(\"local[*]\"))\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the first thing we need to ensure is that we are on the latest version of SystemML, which is 1.3.0 (as of 20th March'19) Please use the code block below to check if you are already on 1.3.0 or higher. 1.3 contains a necessary fix, that's we are running against the SNAPSHOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists('~\\home\\dsxuser\\work\\systemml'):\n",
    "    ! mkdir -p ~\\home\\dsxuser\\work\\systemml\n",
    "#! mkdir -p ~\\home\\dsxuser\\work\\systemml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.0-SNAPSHOT\n"
     ]
    }
   ],
   "source": [
    "from systemml import MLContext, dml\n",
    "ml = MLContext(spark)\n",
    "ml.setConfigProperty(\"sysml.localtmpdir\", \"mkdir /home/dsxuser/work/systemml\")\n",
    "print(ml.version())\n",
    "    \n",
    "if not ml.version() == '1.3.0-SNAPSHOT':\n",
    "    raise ValueError('please upgrade to SystemML 1.3.0, or restart your Kernel (Kernel->Restart & Clear Output)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "package = \"wget\"\n",
    "try:\n",
    "    __import__package\n",
    "except:\n",
    "    #! pip install wget\n",
    "    os.system(\"pip install \"+ package)\n",
    "    import wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                          ]     0 / 74727\r",
      " 10% [......                                                    ]  8192 / 74727\r",
      " 21% [............                                              ] 16384 / 74727\r",
      " 32% [...................                                       ] 24576 / 74727\r",
      " 43% [.........................                                 ] 32768 / 74727\r",
      " 54% [...............................                           ] 40960 / 74727\r",
      " 65% [......................................                    ] 49152 / 74727\r",
      " 76% [............................................              ] 57344 / 74727\r",
      " 87% [..................................................        ] 65536 / 74727\r",
      " 98% [......................................................... ] 73728 / 74727\r",
      "100% [..........................................................] 74727 / 74727"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'shake.parquet'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://github.com/IBM/coursera/blob/master/coursera_ml/shake.parquet?raw=true'\n",
    "\n",
    "if os.path.exists(\"shake.parquet\"):\n",
    "    os.remove(\"shake.parquet\")\n",
    "\n",
    "wget.download(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to read the sensor data and create a temporary query table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=spark.read.parquet('shake.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----+-----+-----+\n",
      "|CLASS| SENSORID|    X|    Y|    Z|\n",
      "+-----+---------+-----+-----+-----+\n",
      "|    2| qqqqqqqq| 0.12| 0.12| 0.12|\n",
      "|    2|aUniqueID| 0.03| 0.03| 0.03|\n",
      "|    2| qqqqqqqq|-3.84|-3.84|-3.84|\n",
      "|    2| 12345678| -0.1| -0.1| -0.1|\n",
      "|    2| 12345678|-0.15|-0.15|-0.15|\n",
      "|    2| 12345678| 0.47| 0.47| 0.47|\n",
      "|    2| 12345678|-0.06|-0.06|-0.06|\n",
      "|    2| 12345678|-0.09|-0.09|-0.09|\n",
      "|    2| 12345678| 0.21| 0.21| 0.21|\n",
      "|    2| 12345678|-0.08|-0.08|-0.08|\n",
      "|    2| 12345678| 0.44| 0.44| 0.44|\n",
      "|    2|    gholi| 0.76| 0.76| 0.76|\n",
      "|    2|    gholi| 1.62| 1.62| 1.62|\n",
      "|    2|    gholi| 5.81| 5.81| 5.81|\n",
      "|    2| bcbcbcbc| 0.58| 0.58| 0.58|\n",
      "|    2| bcbcbcbc|-8.24|-8.24|-8.24|\n",
      "|    2| bcbcbcbc|-0.45|-0.45|-0.45|\n",
      "|    2| bcbcbcbc| 1.03| 1.03| 1.03|\n",
      "|    2|aUniqueID|-0.05|-0.05|-0.05|\n",
      "|    2| qqqqqqqq|-0.44|-0.44|-0.44|\n",
      "+-----+---------+-----+-----+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pixiedust database opened successfully\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div style=\"margin:10px\">\n",
       "            <a href=\"https://github.com/ibm-watson-data-lab/pixiedust\" target=\"_new\">\n",
       "                <img src=\"https://github.com/ibm-watson-data-lab/pixiedust/raw/master/docs/_static/pd_icon32.png\" style=\"float:left;margin-right:10px\"/>\n",
       "            </a>\n",
       "            <span>Pixiedust version 1.1.19</span>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#!pip install --user --upgrade pixiedust\n",
    "package = \"pixiedust\"\n",
    "try:\n",
    "    __import__package\n",
    "except:\n",
    "    os.system(\"pip install \"+ package)\n",
    "    import pixiedust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[CLASS: bigint, SENSORID: string, X: double, Y: double, Z: double]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pixiedust\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"df\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We’ll use Apache SystemML to implement Discrete Fourier Transformation. This way all computation continues to happen on the Apache Spark cluster for advanced scalability and performance.\n",
    "\n",
    "As you’ve learned from the lecture, implementing Discrete Fourier Transformation in a linear algebra programming language is simple. Apache SystemML DML is such a language and as you can see the implementation is straightforward and doesn’t differ too much from the mathematical definition (Just note that the sum operator has been swapped with a vector dot product using the %*% syntax borrowed from R ):\n",
    "\n",
    "$X_k = \\sum_{n=0}^{N_1} x_n . e^{-2\\pi i k n/N}$\n",
    "\n",
    "$=\\sum_{n=0}^{N-1} x_n [cos(2 \\pi k n/N) - i sin(2 \\pi k n/N)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dml_script = '''\n",
    "PI = 3.141592654\n",
    "N = nrow(signal)\n",
    "\n",
    "n = seq(0, N-1, 1)\n",
    "k = seq(0, N-1, 1)\n",
    "\n",
    "M = (n %*% t(k))*(2*PI/N)\n",
    "\n",
    "Xa = cos(M) %*% signal\n",
    "Xb = sin(M) %*% signal\n",
    "\n",
    "DFT = cbind(Xa, Xb)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it’s time to create a function which takes a single row Apache Spark data frame as argument (the one containing the accelerometer measurement time series for one axis) and returns the Fourier transformation of it. In addition, we are adding an index column for later joining all axis together and renaming the columns to appropriate names. The result of this function is an Apache Spark DataFrame containing the Fourier Transformation of its input in two columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "def dft_systemml(signal,name):\n",
    "\n",
    "    prog = dml(dml_script).input('signal', signal).output('DFT')\n",
    "    \n",
    "    return (\n",
    "\n",
    "    #execute the script inside the SystemML engine running on top of Apache Spark\n",
    "    ml.execute(prog) \n",
    "     \n",
    "         #read result from SystemML execution back as SystemML Matrix\n",
    "        .get('DFT') \n",
    "     \n",
    "         #convert SystemML Matrix to ApacheSpark DataFrame \n",
    "        .toDF() \n",
    "     \n",
    "         #rename default column names\n",
    "        .selectExpr('C1 as %sa' % (name), 'C2 as %sb' % (name)) \n",
    "     \n",
    "         #add unique ID per row for later joining\n",
    "        .withColumn(\"id\", monotonically_increasing_id())\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Now it’s time to create individual DataFrames containing only a subset of the data. We filter simultaneously for accelerometer each sensor axis and one for each class. This means you’ll get 6 DataFrames. Please implement this using the relational API of DataFrames or SparkSQL. Please use class 1 and 2 and not 0 and 1.\n",
    "\n",
    "Please make sure that each DataFrame has only ONE colum (only the measurement, eg. not CLASS column)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x0= spark.sql(\"SELECT X from df WHERE CLASS=1\")\n",
    "y0= spark.sql(\"SELECT Y from df WHERE CLASS=1\")\n",
    "z0= spark.sql(\"SELECT Z from df WHERE CLASS=1\")\n",
    "\n",
    "x1= spark.sql(\"SELECT X from df WHERE CLASS=2\")\n",
    "y1= spark.sql(\"SELECT Y from df WHERE CLASS=2\")\n",
    "z1= spark.sql(\"SELECT Z from df WHERE CLASS=2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Since we’ve created this cool DFT function before, we can just call it for each of the 6 DataFrames now. And since the result of this function call is a DataFrame again we can use the pyspark best practice in simply calling methods on it sequentially. So what we are doing is the following:\n",
    "\n",
    "    Calling DFT for each class and accelerometer sensor axis.\n",
    "    Joining them together on the ID column.\n",
    "    Re-adding a column containing the class index.\n",
    "    Stacking both Dataframes for each classes together\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.455 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.115 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t0.107 sec.\n",
      "Number of executed Spark inst:\t0.\n",
      "\n",
      "\n",
      "[Stage 18:>                                                         (0 + 8) / 8]\n",
      "[Stage 18:===========================================>              (6 + 2) / 8]\n",
      "                                                                                \n",
      "[Stage 20:>                                                         (0 + 8) / 8]\n",
      "[Stage 20:==================================================>       (7 + 1) / 8]\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t8.395 sec.\n",
      "Number of executed Spark inst:\t6.\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 26:>                                                         (0 + 8) / 8]\n",
      "[Stage 26:====================================>                     (5 + 3) / 8]\n",
      "[Stage 26:==================================================>       (7 + 1) / 8]\n",
      "[Stage 28:>                                                         (0 + 8) / 8]\n",
      "[Stage 28:===========================================>              (6 + 2) / 8]\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t8.836 sec.\n",
      "Number of executed Spark inst:\t6.\n",
      "\n",
      "                                                                                \n",
      "\n",
      "[Stage 34:>                                                         (0 + 8) / 8]\n",
      "[Stage 34:===========================================>              (6 + 2) / 8]\n",
      "[Stage 34:==================================================>       (7 + 1) / 8]\n",
      "[Stage 36:>                                                         (0 + 8) / 8]\n",
      "[Stage 36:===========================================>              (6 + 2) / 8]\n",
      "SystemML Statistics:\n",
      "Total execution time:\t\t7.846 sec.\n",
      "Number of executed Spark inst:\t6.\n",
      "\n",
      "                                                                                \n",
      "\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|         id|                  xa|                  xb|                  ya|                  yb|                  za|                  zb|class|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "|17179869191|-0.03750866686245739|-0.00607638770223...|-0.03750866686245739|-0.00607638770223...|-0.03750866686245739|-0.00607638770223...|    0|\n",
      "| 8589934592| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539|    0|\n",
      "|34359738369|-0.13783372334014835|-0.06305345056979533|-0.13783372334014835|-0.06305345056979533|-0.13783372334014835|-0.06305345056979533|    0|\n",
      "|          0|-0.03232568625987...| 0.08552771465661582|-0.03232568625987...| 0.08552771465661582|-0.03232568625987...| 0.08552771465661582|    0|\n",
      "|25769803790|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|    0|\n",
      "|25769803780| 0.04131895946735583| 0.02405619286297824| 0.04131895946735583| 0.02405619286297824| 0.04131895946735583| 0.02405619286297824|    0|\n",
      "| 8589934596|-0.07494632636975795| 0.07987898830239344|-0.07494632636975795| 0.07987898830239344|-0.07494632636975795| 0.07987898830239344|    0|\n",
      "| 8589934599| 0.00591146441475935| 0.07661296088428358| 0.00591146441475935| 0.07661296088428358| 0.00591146441475935| 0.07661296088428358|    0|\n",
      "|42949672966| -0.1433589088393247|-0.04024764704794...| -0.1433589088393247|-0.04024764704794...| -0.1433589088393247|-0.04024764704794...|    0|\n",
      "|25769803789|  0.0877424196298842|-0.04399627953075756|  0.0877424196298842|-0.04399627953075756|  0.0877424196298842|-0.04399627953075756|    0|\n",
      "|51539607563|-0.00895316189594...| -0.0969949270045784|-0.00895316189594...| -0.0969949270045784|-0.00895316189594...| -0.0969949270045784|    0|\n",
      "|          7|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|    0|\n",
      "|25769803787| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468|    0|\n",
      "| 8589934598|-0.00210545496225463|-0.00516917411410...|-0.00210545496225463|-0.00516917411410...|-0.00210545496225463|-0.00516917411410...|    0|\n",
      "|42949672971|-0.06864428402270273|-0.06538926569940606|-0.06864428402270273|-0.06538926569940606|-0.06864428402270273|-0.06538926569940606|    0|\n",
      "|51539607565|   0.087742414942036| 0.04399628388176639|   0.087742414942036| 0.04399628388176639|   0.087742414942036| 0.04399628388176639|    0|\n",
      "|25769803786| 0.06623586736304733| 0.01922025213327444| 0.06623586736304733| 0.01922025213327444| 0.06623586736304733| 0.01922025213327444|    0|\n",
      "|42949672965|-0.04728754801118949| 0.07005979666229584|-0.04728754801118949| 0.07005979666229584|-0.04728754801118949| 0.07005979666229584|    0|\n",
      "|42949672970|-0.01014654201362...|-0.05732756250962963|-0.01014654201362...|-0.05732756250962963|-0.01014654201362...|-0.05732756250962963|    0|\n",
      "|34359738373| 0.08065616290607885|0.036400766094453745| 0.08065616290607885|0.036400766094453745| 0.08065616290607885|0.036400766094453745|    0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import lit\n",
    "df_class_0 = dft_systemml(x0,'x') \\\n",
    "    .join(dft_systemml(y0,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemml(z0,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(0))\n",
    "\n",
    "df_class_1 = dft_systemml(x1,'x') \\\n",
    "    .join(dft_systemml(y1,'y'), on=['id'], how='inner') \\\n",
    "    .join(dft_systemml(z1,'z'), on=['id'], how='inner') \\\n",
    "    .withColumn('class', lit(1))\n",
    "    \n",
    "df_dft = df_class_0.union(df_class_1)\n",
    "\n",
    "df_dft.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please create a VectorAssembler which consumes the newly created DFT columns and produces a column “features”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "###YOUR_CODE_GOES_HERE### \n",
    "vectorAssembler = VectorAssembler(inputCols=[\"xa\",\"xb\",\"ya\",\"yb\",\"za\",\"zb\"],\n",
    "                                  outputCol=\"features\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please insatiate a classifier from the SparkML package and assign it to the classifier variable. Make sure to set the “class” column as target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import GBTClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = GBTClassifier(labelCol=\"class\", featuresCol=\"features\", maxIter=10)###YOUR_CODE_GOES_HERE###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let’s train and evaluate…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[vectorAssembler, classifier])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pipeline.fit(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(df_dft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|         id|                  xa|                  xb|                  ya|                  yb|                  za|                  zb|class|            features|       rawPrediction|         probability|prediction|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "|17179869191|-0.03750866686245739|-0.00607638770223...|-0.03750866686245739|-0.00607638770223...|-0.03750866686245739|-0.00607638770223...|    0|[-0.0375086668624...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 8589934592| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539| -0.0213952544564988| -0.1156058071676539|    0|[-0.0213952544564...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|34359738369|-0.13783372334014835|-0.06305345056979533|-0.13783372334014835|-0.06305345056979533|-0.13783372334014835|-0.06305345056979533|    0|[-0.1378337233401...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "|          0|-0.03232568625987...| 0.08552771465661582|-0.03232568625987...| 0.08552771465661582|-0.03232568625987...| 0.08552771465661582|    0|[-0.0323256862598...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "|25769803790|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|0.006260524476137005|-0.05765058448048809|    0|[0.00626052447613...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|25769803780| 0.04131895946735583| 0.02405619286297824| 0.04131895946735583| 0.02405619286297824| 0.04131895946735583| 0.02405619286297824|    0|[0.04131895946735...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 8589934596|-0.07494632636975795| 0.07987898830239344|-0.07494632636975795| 0.07987898830239344|-0.07494632636975795| 0.07987898830239344|    0|[-0.0749463263697...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "| 8589934599| 0.00591146441475935| 0.07661296088428358| 0.00591146441475935| 0.07661296088428358| 0.00591146441475935| 0.07661296088428358|    0|[0.00591146441475...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|42949672966| -0.1433589088393247|-0.04024764704794...| -0.1433589088393247|-0.04024764704794...| -0.1433589088393247|-0.04024764704794...|    0|[-0.1433589088393...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "|25769803789|  0.0877424196298842|-0.04399627953075756|  0.0877424196298842|-0.04399627953075756|  0.0877424196298842|-0.04399627953075756|    0|[0.08774241962988...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|51539607563|-0.00895316189594...| -0.0969949270045784|-0.00895316189594...| -0.0969949270045784|-0.00895316189594...| -0.0969949270045784|    0|[-0.0089531618959...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|          7|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|  0.0334907666040125|  0.0703175032167542|    0|[0.03349076660401...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|25769803787| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468| -0.0189063133643019|-0.08871621465767468|    0|[-0.0189063133643...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "| 8589934598|-0.00210545496225463|-0.00516917411410...|-0.00210545496225463|-0.00516917411410...|-0.00210545496225463|-0.00516917411410...|    0|[-0.0021054549622...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|42949672971|-0.06864428402270273|-0.06538926569940606|-0.06864428402270273|-0.06538926569940606|-0.06864428402270273|-0.06538926569940606|    0|[-0.0686442840227...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "|51539607565|   0.087742414942036| 0.04399628388176639|   0.087742414942036| 0.04399628388176639|   0.087742414942036| 0.04399628388176639|    0|[0.08774241494203...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|25769803786| 0.06623586736304733| 0.01922025213327444| 0.06623586736304733| 0.01922025213327444| 0.06623586736304733| 0.01922025213327444|    0|[0.06623586736304...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|42949672965|-0.04728754801118949| 0.07005979666229584|-0.04728754801118949| 0.07005979666229584|-0.04728754801118949| 0.07005979666229584|    0|[-0.0472875480111...|[0.83002855223973...|[0.84024566850429...|       0.0|\n",
      "|42949672970|-0.01014654201362...|-0.05732756250962963|-0.01014654201362...|-0.05732756250962963|-0.01014654201362...|-0.05732756250962963|    0|[-0.0101465420136...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "|34359738373| 0.08065616290607885|0.036400766094453745| 0.08065616290607885|0.036400766094453745| 0.08065616290607885|0.036400766094453745|    0|[0.08065616290607...|[0.88066422361181...|[0.85337596077307...|       0.0|\n",
      "+-----------+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9981761070017225"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "binEval = MulticlassClassificationEvaluator().setMetricName(\"accuracy\") .setPredictionCol(\"prediction\").setLabelCol(\"class\")\n",
    "    \n",
    "binEval.evaluate(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are happy with the result (I’m happy with > 0.8) please submit your solution to the grader by executing the following cells, please don’t forget to obtain an assignment submission token (secret) from the Courera’s graders web page and paste it to the “secret” variable below, including your email address you’ve used for Coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!rm -Rf a2_m4.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(\"a2_m4.json\"):\n",
    "    os.remove(\"a2_m4.json\")\n",
    "\n",
    "#!del -Rf a2_m4.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = prediction.repartition(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o378.json.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:545)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1454.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1454.0 (TID 51043, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\Bilgi\\OneDrive\\Documents\\Work\\Bilgin\\Learning\\Coursera\\DSP_ADVANCED_ML\\week4\\CourseraML_AssignmentML4\\a2_m4.json\\_temporary\\0\\_temporary\\attempt_20210206233626_1454_m_000000_51043\\part-00000-0c7ff995-ac6d-4a4f-8dfc-b70c6b7f8330-c000.json\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonFileFormat.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\Bilgi\\OneDrive\\Documents\\Work\\Bilgin\\Learning\\Coursera\\DSP_ADVANCED_ML\\week4\\CourseraML_AssignmentML4\\a2_m4.json\\_temporary\\0\\_temporary\\attempt_20210206233626_1454_m_000000_51043\\part-00000-0c7ff995-ac6d-4a4f-8dfc-b70c6b7f8330-c000.json\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonFileFormat.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-50d8506512d8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a2_m4.json'\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# Py4JJavaError: An error occurred while calling o449.json\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[1;34m(self, path, mode, compression, dateFormat, timestampFormat, lineSep, encoding)\u001b[0m\n\u001b[0;32m    815\u001b[0m             \u001b[0mcompression\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdateFormat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdateFormat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimestampFormat\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimestampFormat\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m             lineSep=lineSep, encoding=encoding)\n\u001b[1;32m--> 817\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    819\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0msince\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1.4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1257\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1259\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 328\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    329\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o378.json.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:198)\r\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:159)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\r\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\r\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\r\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\r\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\r\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)\r\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:285)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:271)\r\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:229)\r\n\tat org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:545)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 1454.0 failed 1 times, most recent failure: Lost task 0.0 in stage 1454.0 (TID 51043, localhost, executor driver): java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\Bilgi\\OneDrive\\Documents\\Work\\Bilgin\\Learning\\Coursera\\DSP_ADVANCED_ML\\week4\\CourseraML_AssignmentML4\\a2_m4.json\\_temporary\\0\\_temporary\\attempt_20210206233626_1454_m_000000_51043\\part-00000-0c7ff995-ac6d-4a4f-8dfc-b70c6b7f8330-c000.json\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonFileFormat.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1891)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1879)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1878)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1878)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:927)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:927)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2112)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2061)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2050)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:738)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)\r\n\t... 33 more\r\nCaused by: java.io.IOException: (null) entry in command string: null chmod 0644 C:\\Users\\Bilgi\\OneDrive\\Documents\\Work\\Bilgin\\Learning\\Coursera\\DSP_ADVANCED_ML\\week4\\CourseraML_AssignmentML4\\a2_m4.json\\_temporary\\0\\_temporary\\attempt_20210206233626_1454_m_000000_51043\\part-00000-0c7ff995-ac6d-4a4f-8dfc-b70c6b7f8330-c000.json\r\n\tat org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:770)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:866)\r\n\tat org.apache.hadoop.util.Shell.execCommand(Shell.java:849)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:733)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:225)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$LocalFSFileOutputStream.<init>(RawLocalFileSystem.java:209)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.createOutputStreamWithMode(RawLocalFileSystem.java:307)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:296)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem.create(RawLocalFileSystem.java:328)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem$ChecksumFSOutputSummer.<init>(ChecksumFileSystem.java:398)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:461)\r\n\tat org.apache.hadoop.fs.ChecksumFileSystem.create(ChecksumFileSystem.java:440)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:911)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:892)\r\n\tat org.apache.hadoop.fs.FileSystem.create(FileSystem.java:789)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStream(CodecStreams.scala:81)\r\n\tat org.apache.spark.sql.execution.datasources.CodecStreams$.createOutputStreamWriter(CodecStreams.scala:92)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.<init>(JsonFileFormat.scala:183)\r\n\tat org.apache.spark.sql.execution.datasources.json.JsonFileFormat$$anon$1.newInstance(JsonFileFormat.scala:82)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.newOutputWriter(FileFormatDataWriter.scala:120)\r\n\tat org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.<init>(FileFormatDataWriter.scala:108)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.org$apache$spark$sql$execution$datasources$FileFormatWriter$$executeTask(FileFormatWriter.scala:236)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:170)\r\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$$anonfun$write$1.apply(FileFormatWriter.scala:169)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:123)\r\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "prediction.write.json('a2_m4.json')  # Py4JJavaError: An error occurred while calling o449.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DF to PD and write json\n",
    "prediction_pd = prediction.toPandas()\n",
    "prediction_pd.to_csv('prediction.csv') # inspect the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>xa</th>\n",
       "      <th>xb</th>\n",
       "      <th>ya</th>\n",
       "      <th>yb</th>\n",
       "      <th>za</th>\n",
       "      <th>zb</th>\n",
       "      <th>class</th>\n",
       "      <th>features</th>\n",
       "      <th>rawPrediction</th>\n",
       "      <th>probability</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17179869191</td>\n",
       "      <td>-0.037509</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>-0.037509</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>-0.037509</td>\n",
       "      <td>-0.006076</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.03750866686245739, -0.006076387702233552, ...</td>\n",
       "      <td>[0.8300285522397316, -0.8300285522397316]</td>\n",
       "      <td>[0.8402456685042936, 0.1597543314957064]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8589934592</td>\n",
       "      <td>-0.021395</td>\n",
       "      <td>-0.115606</td>\n",
       "      <td>-0.021395</td>\n",
       "      <td>-0.115606</td>\n",
       "      <td>-0.021395</td>\n",
       "      <td>-0.115606</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.0213952544564988, -0.1156058071676539, -0....</td>\n",
       "      <td>[0.8806642236118194, -0.8806642236118194]</td>\n",
       "      <td>[0.8533759607730729, 0.14662403922692713]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>34359738369</td>\n",
       "      <td>-0.137834</td>\n",
       "      <td>-0.063053</td>\n",
       "      <td>-0.137834</td>\n",
       "      <td>-0.063053</td>\n",
       "      <td>-0.137834</td>\n",
       "      <td>-0.063053</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.13783372334014835, -0.06305345056979533, -...</td>\n",
       "      <td>[0.8300285522397316, -0.8300285522397316]</td>\n",
       "      <td>[0.8402456685042936, 0.1597543314957064]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>-0.032326</td>\n",
       "      <td>0.085528</td>\n",
       "      <td>-0.032326</td>\n",
       "      <td>0.085528</td>\n",
       "      <td>-0.032326</td>\n",
       "      <td>0.085528</td>\n",
       "      <td>0</td>\n",
       "      <td>[-0.032325686259876626, 0.08552771465661582, -...</td>\n",
       "      <td>[0.8300285522397316, -0.8300285522397316]</td>\n",
       "      <td>[0.8402456685042936, 0.1597543314957064]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>25769803790</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>-0.057651</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>-0.057651</td>\n",
       "      <td>0.006261</td>\n",
       "      <td>-0.057651</td>\n",
       "      <td>0</td>\n",
       "      <td>[0.006260524476137005, -0.05765058448048809, 0...</td>\n",
       "      <td>[0.8806642236118194, -0.8806642236118194]</td>\n",
       "      <td>[0.8533759607730729, 0.14662403922692713]</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id        xa        xb        ya        yb        za        zb  \\\n",
       "0  17179869191 -0.037509 -0.006076 -0.037509 -0.006076 -0.037509 -0.006076   \n",
       "1   8589934592 -0.021395 -0.115606 -0.021395 -0.115606 -0.021395 -0.115606   \n",
       "2  34359738369 -0.137834 -0.063053 -0.137834 -0.063053 -0.137834 -0.063053   \n",
       "3            0 -0.032326  0.085528 -0.032326  0.085528 -0.032326  0.085528   \n",
       "4  25769803790  0.006261 -0.057651  0.006261 -0.057651  0.006261 -0.057651   \n",
       "\n",
       "   class                                           features  \\\n",
       "0      0  [-0.03750866686245739, -0.006076387702233552, ...   \n",
       "1      0  [-0.0213952544564988, -0.1156058071676539, -0....   \n",
       "2      0  [-0.13783372334014835, -0.06305345056979533, -...   \n",
       "3      0  [-0.032325686259876626, 0.08552771465661582, -...   \n",
       "4      0  [0.006260524476137005, -0.05765058448048809, 0...   \n",
       "\n",
       "                               rawPrediction  \\\n",
       "0  [0.8300285522397316, -0.8300285522397316]   \n",
       "1  [0.8806642236118194, -0.8806642236118194]   \n",
       "2  [0.8300285522397316, -0.8300285522397316]   \n",
       "3  [0.8300285522397316, -0.8300285522397316]   \n",
       "4  [0.8806642236118194, -0.8806642236118194]   \n",
       "\n",
       "                                 probability  prediction  \n",
       "0   [0.8402456685042936, 0.1597543314957064]         0.0  \n",
       "1  [0.8533759607730729, 0.14662403922692713]         0.0  \n",
       "2   [0.8402456685042936, 0.1597543314957064]         0.0  \n",
       "3   [0.8402456685042936, 0.1597543314957064]         0.0  \n",
       "4  [0.8533759607730729, 0.14662403922692713]         0.0  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_pd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "prediction2Json = prediction_pd.to_json(orient=\"split\")\n",
    "predJsonParsed = json.loads(prediction2Json)\n",
    "json.dumps(predJsonParsed)  \n",
    "\n",
    "with open('a2_m4.json', 'w') as json_file:\n",
    "    json.dump(predJsonParsed, json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0% [                                                            ]    0 / 2540\r",
      "100% [............................................................] 2540 / 2540"
     ]
    }
   ],
   "source": [
    "import os\n",
    "url = 'https://raw.githubusercontent.com/IBM/coursera/master/rklib.py'\n",
    "if os.path.exists(\"rklib.py\"):\n",
    "  os.remove(\"rklib.py\")\n",
    "else:\n",
    "  wget.download(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from rklib import zipit\n",
    "#zipit('a2_m4.json.zip','a2_m4.json')\n",
    "# it does not copy the json file to zipfolder. I use the following instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "with zipfile.ZipFile('a2_m4.json.zip', 'w') as myzip:\n",
    "    myzip.write('a2_m4.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'base64' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "#!base64 \"a2_m4.json.zip\" > \"a2_m4.json.zip.base64\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "with open(\"a2_m4.json.zip\", \"rb\") as f:\n",
    "    encodedZip = base64.b64encode(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('a2_m4.json.zip.base64', 'wb') as f:\n",
    "    f.write(encodedZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission successful, please check on the coursera grader page for the status\n",
      "-------------------------\n",
      "{\"elements\":[{\"itemId\":\"B8wXV\",\"id\":\"f_F-qCtuEei_fRLwaVDk3g~B8wXV~i6Tpw2j8EeuU7g7KNfA_2w\",\"courseId\":\"f_F-qCtuEei_fRLwaVDk3g\"}],\"paging\":{},\"linked\":{}}\n",
      "-------------------------\n"
     ]
    }
   ],
   "source": [
    "from rklib import submit\n",
    "key = \"-fBiYHYDEeiR4QqiFhAvkA\"\n",
    "part = \"IjtJk\"\n",
    "email = 'baltundas@gmail.com'###YOUR_CODE_GOES_HERE###\n",
    "submission_token = 'IQKAAMOlmVthdXV7'###YOUR_CODE_GOES_HERE### # (have a look here if you need more information on how to obtain the token https://youtu.be/GcDo0Rwe06U?t=276)\n",
    "\n",
    "with open('a2_m4.json.zip.base64', 'r') as myfile:\n",
    "    data=myfile.read()\n",
    "submit(email, submission_token, key, part, [part], data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
